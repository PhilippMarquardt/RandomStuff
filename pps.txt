import polars as pl
import json
from typing import Any, Dict, List, Optional, Tuple

# ======================================================================
# 1. SIMULATED DATABASE LAYER
# ======================================================================
class MockDatabaseService:
    """
    Simulates fetching data from your production database.
    """
    @staticmethod
    def fetch_instrument_attributes(instrument_ids: List[int]) -> pl.DataFrame:
        """
        Simulates: SELECT id, sector, currency, asset_class FROM instruments WHERE id IN (...)
        """
        # Mock DB Data
        db_data = {
            100: {"sector": "Technology", "currency": "USD", "asset_class": "Equity"},     # The Fund
            200: {"sector": "Government", "currency": "USD", "asset_class": "Cash"},       # Top-level Cash
            999: {"sector": "Technology", "currency": "USD", "asset_class": "Equity"},     # Tech Stock
            888: {"sector": "Communication", "currency": "USD", "asset_class": "Equity"},  # Comm Stock
            666: {"sector": "Energy", "currency": "RUB", "asset_class": "Equity"},         # Sanctioned Entity
        }
        
        rows = []
        for i_id in instrument_ids:
            # Default if not found in DB
            attrs = db_data.get(i_id, {"sector": "Other", "currency": "USD", "asset_class": "Other"})
            attrs["instrument_identifier"] = i_id
            rows.append(attrs)
            
        return pl.DataFrame(rows)

    @staticmethod
    def fetch_perspective_rules(config_id: str) -> List[Dict[str, Any]]:
        """
        Simulates fetching the JSON rule configuration from the DB.
        """
        # Config 404294674: "Exclude Cash and Sanctioned Entities"
        if config_id == "404294674":
            return [
                # Rule 1: Remove Instrument 666
                {
                    "criteria": {"column": "instrument_identifier", "operator_type": "!=", "value": 666}, 
                    "apply_to": "both",
                    "condition_for_next_rule": "And"
                },
                # Rule 2: Remove Asset Class 'Cash'
                {
                    "criteria": {"column": "asset_class", "operator_type": "!=", "value": "Cash"}, 
                    "apply_to": "both"
                }
            ]
        return []

# ======================================================================
# 2. LOGIC & EXPRESSION BUILDER (Optimized)
# ======================================================================
class PolarsExpressionBuilder:
    """
    Converts JSON criteria into optimized Polars Expressions.
    """
    @staticmethod
    def normalize_value(val: Any, op: str) -> Any:
        if op in ("in", "notin", "between", "notbetween") and isinstance(val, str):
            parts = val.strip("[]").split(",")
            normalized = []
            for x in parts:
                x = x.strip()
                if x.isdigit(): normalized.append(int(x))
                else: normalized.append(x.lower() if isinstance(x, str) else x)
            return normalized
        elif isinstance(val, str): return val.lower()
        return val

    @staticmethod
    def _get_col_expr(col_name: str, as_string: bool = False) -> pl.Expr:
        c = pl.col(col_name)
        if as_string: return c.cast(pl.Utf8).str.to_lowercase()
        return c

    @staticmethod
    def build_rules_expr(rules: List[Dict[str, Any]]) -> pl.Expr:
        if not rules: return pl.lit(True)
        combined_expr = None
        
        for i, rule in enumerate(rules):
            criteria = rule.get("criteria", {})
            col_name = criteria["column"]
            op = criteria.get("operator_type", "").lower()
            raw_val = criteria.get("value")
            
            # Null checks
            if op == "isnull": curr = pl.col(col_name).is_null()
            elif op == "isnotnull": curr = pl.col(col_name).is_not_null()
            else:
                val = PolarsExpressionBuilder.normalize_value(raw_val, op)
                is_str_op = isinstance(val, str) or (isinstance(val, list) and any(isinstance(x, str) for x in val))
                col = PolarsExpressionBuilder._get_col_expr(col_name, as_string=is_str_op)
                
                if op == "=": curr = col.eq(val)
                elif op == "!=": curr = col.ne(val)
                elif op == ">": curr = col.gt(val)
                elif op == "<": curr = col.lt(val)
                elif op == "in": 
                    if isinstance(val, list): curr = col.is_in(val)
                    else: curr = col.eq(val)
                else: curr = pl.lit(False)

            # Apply To Logic
            apply_to = (rule.get("apply_to") or "both").lower()
            if apply_to == "holding": curr = curr & pl.col("category").eq("holding")
            elif apply_to == "reference": curr = curr & pl.col("category").eq("reference")
            
            # Chaining Logic (AND/OR)
            if i == 0: combined_expr = curr
            else:
                prev_rule = rules[i-1]
                cond = prev_rule.get("condition_for_next_rule", "And").lower()
                combined_expr = combined_expr | curr if cond == "or" else combined_expr & curr
                
        return combined_expr if combined_expr is not None else pl.lit(True)

# ======================================================================
# 3. MAIN PIPELINE CLASS (Dynamic Parsing)
# ======================================================================
class PerspectivePipeline:
    def __init__(self):
        self.df: pl.DataFrame = pl.DataFrame()

    def load_perspective_json(self, json_data: Dict[str, Any]) -> 'PerspectivePipeline':
        """
        Parses nested JSON into a flat DataFrame.
        Dynamically detects containers by checking for the 'positions' key.
        """
        dfs = []
        def extract(data_dict, extra_metadata):
            if not data_dict: return
            rows = []
            for node_id, attrs in data_dict.items():
                row = attrs.copy()
                row['node_id'] = str(node_id)
                row.update(extra_metadata)
                rows.append(row)
            if rows: dfs.append(pl.DataFrame(rows, infer_schema_length=None))

        # --- DYNAMIC SCANNING LOGIC ---
        for key, value in json_data.items():
            # 1. Is it a potential container? (Must be a dict)
            if not isinstance(value, dict):
                continue
                
            # 2. Does it look like perspective data? (Must contain 'positions')
            if "positions" in value:
                container_key = key
                # Heuristic: "holding" is holding, everything else is "reference"
                # This allows 'contractual_reference', 'custom_reference_1', etc.
                category_label = "holding" if container_key == "holding" else "reference"
                
                # Extract Positions
                extract(value.get("positions"), {
                    "category": category_label, 
                    "container": container_key, 
                    "record_type": "position"
                })
                
                # Extract Lookthroughs
                extract(value.get("complete_lookthroughs"), {
                    "category": category_label, 
                    "container": container_key, 
                    "record_type": "lookthrough"
                })

        if dfs:
            self.df = pl.concat(dfs, how="diagonal")
            # Ensure integer IDs for joining
            for col in ["instrument_identifier", "parent_instrument_id"]:
                if col in self.df.columns:
                    self.df = self.df.with_columns(pl.col(col).cast(pl.Int64, strict=False))
        return self

    def enrich_from_db(self) -> 'PerspectivePipeline':
        """
        Joins DB attributes (Sector, Asset Class, etc.)
        """
        if self.df.is_empty() or "instrument_identifier" not in self.df.columns: return self
        
        unique_ids = self.df["instrument_identifier"].unique().to_list()
        attributes_df = MockDatabaseService.fetch_instrument_attributes(unique_ids)
        
        self.df = self.df.join(attributes_df, on="instrument_identifier", how="left")
        return self

    def apply_rules(self, config_id: str) -> 'PerspectivePipeline':
        """
        Fetches rules from DB and applies filtering.
        """
        if self.df.is_empty(): return self
        rules = MockDatabaseService.fetch_perspective_rules(config_id)
        if rules:
            print(f"Applying {len(rules)} rules from config {config_id}...")
            expr = PolarsExpressionBuilder.build_rules_expr(rules)
            self.df = self.df.filter(expr)
        return self

    # --- STAGE 1: Scale Top-Level Positions to 100% ---
    def rescale_positions(self, weight_labels: List[str]) -> 'PerspectivePipeline':
        """
        Calculates sum of all positions in a container.
        Scales them so they sum to 1.0.
        """
        if self.df.is_empty(): return self
        valid_labels = [w for w in weight_labels if w in self.df.columns]

        for label in valid_labels:
            self.df = self.df.with_columns(pl.col(label).cast(pl.Float64, strict=False))
            
            # 1. Calculate Current Sum per Container
            container_sums = self.df.filter(
                pl.col("record_type") == "position"
            ).group_by("container").agg(
                pl.col(label).sum().alias("pos_sum")
            )

            # 2. Calculate Factor (1.0 / Sum)
            factors = container_sums.with_columns(
                (1.0 / pl.col("pos_sum")).fill_nan(1.0).fill_null(1.0).alias("_pos_factor")
            )

            # 3. Apply Factor
            self.df = self.df.join(
                factors, on="container", how="left"
            ).with_columns(
                pl.when(pl.col("record_type") == "position")
                .then(pl.col(label) * pl.col("_pos_factor"))
                .otherwise(pl.col(label))
                .alias(label)
            ).drop(["pos_sum", "_pos_factor"])

        return self

    # --- STAGE 2: Scale Lookthroughs to Parent ---
    def rescale_lookthroughs(self, weight_labels: List[str]) -> 'PerspectivePipeline':
        """
        Ensures lookthroughs sum to their Parent's (newly scaled) weight.
        """
        if self.df.is_empty(): return self
        valid_labels = [w for w in weight_labels if w in self.df.columns]
        
        for label in valid_labels:
            self.df = self.df.with_columns(pl.col(label).cast(pl.Float64, strict=False))

            # A. Targets (The Parents)
            parents = self.df.filter(pl.col("record_type") == "position").select([
                pl.col("instrument_identifier").alias("join_id"),
                pl.col("container"),
                pl.col(label).alias("target")
            ])
            
            # B. Actuals (Sum of Children)
            children_sums = self.df.filter(pl.col("record_type") == "lookthrough") \
                .group_by(["parent_instrument_id", "container"]) \
                .agg(pl.col(label).sum().alias("actual"))

            # C. Factor (Target / Actual)
            factors = parents.join(
                children_sums, 
                left_on=["join_id", "container"], 
                right_on=["parent_instrument_id", "container"], 
                how="inner"
            ).with_columns(
                (pl.col("target") / pl.col("actual")).fill_nan(1.0).fill_null(1.0).alias("_lt_factor")
            )

            # D. Apply
            self.df = self.df.join(
                factors.select(["join_id", "container", "_lt_factor"]),
                left_on=["parent_instrument_id", "container"], 
                right_on=["join_id", "container"], 
                how="left"
            ).with_columns(
                pl.when(pl.col("record_type") == "lookthrough")
                .then(pl.col(label) * pl.col("_lt_factor"))
                .otherwise(pl.col(label))
                .alias(label)
            ).drop("_lt_factor")
            
        return self

# ======================================================================
# 4. EXECUTION SCENARIO
# ======================================================================
if __name__ == "__main__":
    # INPUT DATA SCENARIO:
    # Note the dynamic reference keys: "holding", "contractual_reference_A", "selected_ref_B"
    
    input_json = {
        "holding": {
            "positions": {
                "p1": {"instrument_identifier": 100, "weight": 0.9, "exposure": 900.0},
                "p2": {"instrument_identifier": 200, "weight": 0.1, "exposure": 100.0} 
            },
            "complete_lookthroughs": {
                "lt1": {"instrument_identifier": 999, "parent_instrument_id": 100, "weight": 0.4, "exposure": 400.0},
                "lt2": {"instrument_identifier": 666, "parent_instrument_id": 100, "weight": 0.2, "exposure": 200.0},
                "lt3": {"instrument_identifier": 888, "parent_instrument_id": 100, "weight": 0.3, "exposure": 300.0}
            }
        },
        "contractual_reference_custom_1": {
             "positions": {
                "r1": {"instrument_identifier": 100, "weight": 1.0, "exposure": 1000.0}
             },
             "complete_lookthroughs": {
                 "lt_r1": {"instrument_identifier": 666, "parent_instrument_id": 100, "weight": 1.0, "exposure": 1000.0}
             }
        }
    }
    
    print("--- Starting Pipeline ---")
    
    pipeline = PerspectivePipeline()
    
    # The full logic chain
    df_final = (
        pipeline
        .load_perspective_json(input_json)
        .enrich_from_db()                             # Adds 'asset_class' needed for filtering
        .apply_rules("404294674")                     # Filters Cash (200) and Sanctioned (666)
        .rescale_positions(["weight", "exposure"])    # Scales remaining positions
        .rescale_lookthroughs(["weight", "exposure"]) # Scales remaining lookthroughs
        .df
    )
    
    print("\n--- Final Result ---")
    cols = ["node_id", "record_type", "container", "instrument_identifier", "weight", "exposure"]
    print(df_final.select(cols).sort(["container", "record_type", "node_id"], descending=True))